{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2349fad6-863d-4fe4-9ca7-90f606921069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a raw Delta table with CDC enabled\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS INCREMENTAL_LOAD.DEFAULT.RAW_UPI_TRANSACTIONS (\n",
    "              TRANSACTION_ID STRING,\n",
    "              UPI_ID STRING,\n",
    "              MERCHANT_ID STRING,\n",
    "              TRANSACTION_AMOUNT DOUBLE,\n",
    "              TRANSACTION_TIMESTAMP TIMESTAMP,\n",
    "              TRANSACTION_STATUS STRING\n",
    "          )\n",
    "          USING DELTA\n",
    "          TBLPROPERTIES ('DELTA.ENABLECHANGEDATAFEED' = 'true')\n",
    "\"\"\")\n",
    "print(\"DELTA TABLE 'INCREMENTAL_LOAD.DEFAULT.RAW_UPI_TRANSACTIONS' CREATED WITH CDC ENABLED...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae33b1ac-ead1-4cbc-9d91-ff0504268154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import time\n",
    "\n",
    "# Mock data batches to simulate CDC\n",
    "mock_batches = [\n",
    "    # Batch 1: Insert new transactions\n",
    "    spark.createDataFrame([\n",
    "        (\"T001\", \"upi1@bank\", \"M001\", 500.0, \"2024-12-21 10:00:00\", \"initiated\"),\n",
    "        (\"T002\", \"upi2@bank\", \"M002\", 1000.0, \"2024-12-21 10:05:00\", \"initiated\"),\n",
    "        (\"T003\", \"upi3@bank\", \"M003\", 1500.0, \"2024-12-21 10:10:00\", \"initiated\"),\n",
    "    ], [\"transaction_id\", \"upi_id\", \"merchant_id\", \"transaction_amount\", \"transaction_timestamp\", \"transaction_status\"]),\n",
    "\n",
    "    # Batch 2: Update and insert transactions\n",
    "    spark.createDataFrame([\n",
    "        (\"T001\", \"upi1@bank\", \"M001\", 500.0, \"2024-12-21 10:15:00\", \"completed\"),  # Update transaction\n",
    "        (\"T002\", \"upi2@bank\", \"M002\", 1000.0, \"2024-12-21 10:20:00\", \"failed\"),    # Update transaction\n",
    "        (\"T004\", \"upi4@bank\", \"M004\", 2000.0, \"2024-12-21 10:25:00\", \"initiated\"), # New transaction\n",
    "    ], [\"transaction_id\", \"upi_id\", \"merchant_id\", \"transaction_amount\", \"transaction_timestamp\", \"transaction_status\"]),\n",
    "\n",
    "    # Batch 3: Handle refunds and updates\n",
    "    spark.createDataFrame([\n",
    "        (\"T001\", \"upi1@bank\", \"M001\", 500.0, \"2024-12-21 10:30:00\", \"refunded\"),  # Refund issued\n",
    "        (\"T003\", \"upi3@bank\", \"M003\", 1500.0, \"2024-12-21 10:35:00\", \"completed\"), # Completed transaction\n",
    "    ], [\"transaction_id\", \"upi_id\", \"merchant_id\", \"transaction_amount\", \"transaction_timestamp\", \"transaction_status\"]),\n",
    "]\n",
    "\n",
    "\n",
    "# Merge logic\n",
    "def merge_to_delta_table(delta_table_name: str, batch_df):\n",
    "    delta_table = DeltaTable.forName(spark, delta_table_name)\n",
    "\n",
    "    # Perform merge operation\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        batch_df.alias(\"source\"),\n",
    "        \"target.transaction_id = source.transaction_id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        set={\n",
    "            \"upi_id\": \"source.upi_id\",\n",
    "            \"merchant_id\": \"source.merchant_id\",\n",
    "            \"transaction_amount\": \"source.transaction_amount\",\n",
    "            \"transaction_timestamp\": \"source.transaction_timestamp\",\n",
    "            \"transaction_status\": \"source.transaction_status\"\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# for i, batch_df in enumerate(mock_batches):\n",
    "#     print(f\"Processing batch {i + 1}\")\n",
    "#     merge_to_delta_table(\"INCREMENTAL_LOAD.DEFAULT.RAW_UPI_TRANSACTIONS\", batch_df)\n",
    "#     print(f\"Batch {i + 1} processed successfully.\")\n",
    "#     time.sleep(10)\n",
    "\n",
    "\n",
    "merge_to_delta_table(\"INCREMENTAL_LOAD.DEFAULT.RAW_UPI_TRANSACTIONS\", mock_batches[2])\n",
    "print(f\"Batch processed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UPI_TRANSACTION_MOCK_DATA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
